### Before starting to install Kubernetes
-
  name: 'Install Docker on Worker nodes'
  hosts: worker-*
  tasks:
    -
      name: 'Verify if Docker is installed'
      stat:
        path: /usr/bin/docker
      register: docker_installed
    -
      name: 'Install Docker if not installed'
      ### Install Docker && Add user 'vagrant' to group 'docker'
      shell: 'curl -fsSL https://get.docker.com/ | sh && sudo usermod -aG docker $USER'
      when: docker_installed.stat.islnk is not defined
    -
      name: 'Verify if pip3 is installed'
      stat:
        path: /usr/bin/pip3
      register: pip_installed
    -
      name: 'Install pip3 & Docker SDK if not installed'
      ### Update && Install pip3 && Install Docker SDK && Give access to Docker Socket for Docker API
      shell: 'sudo apt -y update && sudo apt -y install python3-pip && sudo pip3 install docker && sudo chown $USER /var/run/docker.sock'
      when: pip_installed.stat.islnk is not defined
    -
      name: 'Modify Docker daemon to use cgroupdriver systemd'
      shell: |
        cat <<EOF | sudo tee /etc/docker/daemon.json  
        {  
        "exec-opts": ["native.cgroupdriver=systemd"],  
        "log-driver": "json-file",  
        "log-opts": {  
        "max-size": "100m"  
        },  
        "storage-driver": "overlay2"  
        }  
        EOF
        sudo systemctl daemon-reload
        sudo systemctl restart docker
-
  name: 'Allow bridge traffic on Worker nodes'
  hosts: worker-*
  tasks:
    -
      name: 'Allow bridge traffic'
      shell: |
        sudo sysctl net.bridge.bridge-nf-call-iptables=1
-
  name: 'Update /etc/hosts on all nodes'
  hosts: all
  tasks:
    -
      name: 'Append nodes IP addresses to /etc/hosts file'
      shell: |
        cat <<EOF | sudo tee -a /etc/hosts
        192.168.42.1 master-1
        192.168.42.2 master-2
        192.168.42.11 worker-1
        192.168.42.12 worker-2
        192.168.42.21 loadbalancer-1
        EOF
-
  name: 'Create a shared folder accessible for all nodes'
  hosts: master-1
  tasks:
    -
      name: 'Create shared folder'
      shell: 'mkdir -p /vagrant/.vagrant/shared-folder'
## Kubernetes installation
-
  name: 'Install kubectl on Master nodes'
  hosts: master-*
  tasks:
    -
      name: 'Install kubectl v1.13.0'
      shell: |
        wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubectl
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
-
  name: 'Provisioning CA and Generating TLS Certificates'
  hosts: master-1
  tasks:
    -
      name: 'Create CA crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out ca.key 2048
        sudo sed -i '0,/RANDFILE/{s/RANDFILE/\#&/}' /etc/ssl/openssl.cnf
        openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
        openssl x509 -req -in ca.csr -signkey ca.key -CAcreateserial  -out ca.crt -days 1000
    -
      name: 'Create admin crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out admin.key 2048
        openssl req -new -key admin.key -subj "/CN=admin/O=system:masters" -out admin.csr
        openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out admin.crt -days 1000
    -
      name: 'Create kube-controller-manager crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out kube-controller-manager.key 2048
        openssl req -new -key kube-controller-manager.key -subj "/CN=system:kube-controller-manager" -out kube-controller-manager.csr
        openssl x509 -req -in kube-controller-manager.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-controller-manager.crt -days 1000
    -
      name: 'Create kube-proxy crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out kube-proxy.key 2048
        openssl req -new -key kube-proxy.key -subj "/CN=system:kube-proxy" -out kube-proxy.csr
        openssl x509 -req -in kube-proxy.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-proxy.crt -days 1000    
    -
      name: 'Create kube-scheduler crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out kube-scheduler.key 2048
        openssl req -new -key kube-scheduler.key -subj "/CN=system:kube-scheduler" -out kube-scheduler.csr
        openssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-scheduler.crt -days 1000
    -
      name: 'Create openssl configuration file for kube-apiserver'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        cat > openssl.cnf <<EOF
        [req]
        req_extensions = v3_req
        distinguished_name = req_distinguished_name
        [req_distinguished_name]
        [ v3_req ]
        basicConstraints = CA:FALSE
        keyUsage = nonRepudiation, digitalSignature, keyEncipherment
        subjectAltName = @alt_names
        [alt_names]
        DNS.1 = kubernetes
        DNS.2 = kubernetes.default
        DNS.3 = kubernetes.default.svc
        DNS.4 = kubernetes.default.svc.cluster.local
        IP.1 = 10.96.0.1
        IP.2 = 192.168.42.1
        IP.3 = 192.168.42.2
        IP.4 = 192.168.42.21
        IP.5 = 127.0.0.1
        EOF
    -
      name: 'Create kube-apiserver crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out kube-apiserver.key 2048
        openssl req -new -key kube-apiserver.key -subj "/CN=kube-apiserver" -out kube-apiserver.csr -config openssl.cnf
        openssl x509 -req -in kube-apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-apiserver.crt -extensions v3_req -extfile openssl.cnf -days 1000
    -
      name: 'Create openssl configuration file for etcd'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        cat > openssl-etcd.cnf <<EOF
        [req]
        req_extensions = v3_req
        distinguished_name = req_distinguished_name
        [req_distinguished_name]
        [ v3_req ]
        basicConstraints = CA:FALSE
        keyUsage = nonRepudiation, digitalSignature, keyEncipherment
        subjectAltName = @alt_names
        [alt_names]
        IP.1 = 192.168.42.1
        IP.2 = 192.168.42.2
        IP.3 = 127.0.0.1
        EOF
    -
      name: 'Create etcd crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out etcd-server.key 2048
        openssl req -new -key etcd-server.key -subj "/CN=etcd-server" -out etcd-server.csr -config openssl-etcd.cnf
        openssl x509 -req -in etcd-server.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out etcd-server.crt -extensions v3_req -extfile openssl-etcd.cnf -days 1000
    -
      name: 'Create service-account crt, csr and key'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        openssl genrsa -out service-account.key 2048
        openssl req -new -key service-account.key -subj "/CN=service-accounts" -out service-account.csr
        openssl x509 -req -in service-account.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out service-account.crt -days 1000
-
  name: 'Generating Kubernetes configuration files for Authentication'
  hosts: master-1
  tasks:
    -
      name: 'Generate kubeconfig file for kube-proxy'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        {
          kubectl config set-cluster kubernetes-the-hard-way \
            --certificate-authority=ca.crt \
            --embed-certs=true \
            --server=https://192.168.42.21:6443 \
            --kubeconfig=kube-proxy.kubeconfig

          kubectl config set-credentials system:kube-proxy \
            --client-certificate=kube-proxy.crt \
            --client-key=kube-proxy.key \
            --embed-certs=true \
            --kubeconfig=kube-proxy.kubeconfig

          kubectl config set-context default \
            --cluster=kubernetes-the-hard-way \
            --user=system:kube-proxy \
            --kubeconfig=kube-proxy.kubeconfig

          kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
        }
    -
      name: 'Generate kubeconfig file for kube-controller-manager'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        {
          kubectl config set-cluster kubernetes-the-hard-way \
            --certificate-authority=ca.crt \
            --embed-certs=true \
            --server=https://127.0.0.1:6443 \
            --kubeconfig=kube-controller-manager.kubeconfig

          kubectl config set-credentials system:kube-controller-manager \
            --client-certificate=kube-controller-manager.crt \
            --client-key=kube-controller-manager.key \
            --embed-certs=true \
            --kubeconfig=kube-controller-manager.kubeconfig

          kubectl config set-context default \
            --cluster=kubernetes-the-hard-way \
            --user=system:kube-controller-manager \
            --kubeconfig=kube-controller-manager.kubeconfig

          kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
        }
    -
      name: 'Generate kubeconfig file for kube-scheduler'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        {
          kubectl config set-cluster kubernetes-the-hard-way \
            --certificate-authority=ca.crt \
            --embed-certs=true \
            --server=https://127.0.0.1:6443 \
            --kubeconfig=kube-scheduler.kubeconfig

          kubectl config set-credentials system:kube-scheduler \
            --client-certificate=kube-scheduler.crt \
            --client-key=kube-scheduler.key \
            --embed-certs=true \
            --kubeconfig=kube-scheduler.kubeconfig

          kubectl config set-context default \
            --cluster=kubernetes-the-hard-way \
            --user=system:kube-scheduler \
            --kubeconfig=kube-scheduler.kubeconfig

          kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
        }
    -
      name: 'Generate kubeconfig file for admin user'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        {
          kubectl config set-cluster kubernetes-the-hard-way \
            --certificate-authority=ca.crt \
            --embed-certs=true \
            --server=https://127.0.0.1:6443 \
            --kubeconfig=admin.kubeconfig

          kubectl config set-credentials admin \
            --client-certificate=admin.crt \
            --client-key=admin.key \
            --embed-certs=true \
            --kubeconfig=admin.kubeconfig

          kubectl config set-context default \
            --cluster=kubernetes-the-hard-way \
            --user=admin \
            --kubeconfig=admin.kubeconfig

          kubectl config use-context default --kubeconfig=admin.kubeconfig
        }
-
  name: 'Generating the Data Encryption Config and Key'
  hosts: master-1
  tasks:
    -
      name: 'Create encryption-config.yaml file'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
        cat > encryption-config.yaml <<EOF
        kind: EncryptionConfig
        apiVersion: v1
        resources:
          - resources:
              - secrets
            providers:
              - aescbc:
                  keys:
                    - name: key1
                      secret: ${ENCRYPTION_KEY}
              - identity: {}
        EOF
-
  name: 'Applying the Data Encryption Config on Master nodes'
  hosts: master-*
  tasks:
    -
      name: 'Copy encryption-config.yaml file to /var/lib/kubernetes'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        sudo mkdir -p /var/lib/kubernetes
        sudo cp encryption-config.yaml /var/lib/kubernetes/
-
  name: 'Bootstrapping the etcd Cluster'
  hosts: master-*
  tasks:
    -
      name: 'Install etcd Server and etcdctl command line utility'
      shell: |
        wget -q --show-progress --https-only --timestamping \
        "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"
        tar -xvf etcd-v3.3.9-linux-amd64.tar.gz
        sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/
    - 
      name: 'Configure etcd Server'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        sudo mkdir -p /etc/etcd /var/lib/etcd
        sudo cp ca.crt etcd-server.key etcd-server.crt /etc/etcd/
    -
      name: 'Create etcd service'
      shell: |
        INTERNAL_IP=$(ip addr show enp0s8 | grep "inet " | awk '{print $2}' | cut -d / -f 1)
        ETCD_NAME=$(hostname -s)
        cat <<EOF | sudo tee /etc/systemd/system/etcd.service
        [Unit]
        Description=etcd
        Documentation=https://github.com/coreos

        [Service]
        ExecStart=/usr/local/bin/etcd \\
          --name ${ETCD_NAME} \\
          --cert-file=/etc/etcd/etcd-server.crt \\
          --key-file=/etc/etcd/etcd-server.key \\
          --peer-cert-file=/etc/etcd/etcd-server.crt \\
          --peer-key-file=/etc/etcd/etcd-server.key \\
          --trusted-ca-file=/etc/etcd/ca.crt \\
          --peer-trusted-ca-file=/etc/etcd/ca.crt \\
          --peer-client-cert-auth \\
          --client-cert-auth \\
          --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
          --listen-peer-urls https://${INTERNAL_IP}:2380 \\
          --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
          --advertise-client-urls https://${INTERNAL_IP}:2379 \\
          --initial-cluster-token etcd-cluster-0 \\
          --initial-cluster master-1=https://192.168.42.1:2380,master-2=https://192.168.42.2:2380 \\
          --initial-cluster-state new \\
          --data-dir=/var/lib/etcd
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF
        sudo systemctl daemon-reload
        sudo systemctl enable etcd
        sudo systemctl start etcd
-
  name: 'Bootstrapping the Kubernetes Control Plane'
  hosts: master-*
  tasks: 
    -
      name: 'Install Kubernetes binaries'
      shell: |
        sudo mkdir -p /etc/kubernetes/config
        wget -q --show-progress --https-only --timestamping \
        "https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-apiserver" \
        "https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-controller-manager" \
        "https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler" \
        "https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubectl"
        chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
        sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
    -
      name: 'Copy kube-controller-manager kubeconfig to /var/lib/kubernetes'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        sudo cp ca.crt ca.key kube-apiserver.crt kube-apiserver.key \
        service-account.key service-account.crt \
        etcd-server.key etcd-server.crt /var/lib/kubernetes/
    -
      name: 'Create kube-apiserver service'
      shell: |
        INTERNAL_IP=$(ip addr show enp0s8 | grep "inet " | awk '{print $2}' | cut -d / -f 1)
        cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
        [Unit]
        Description=Kubernetes API Server
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        ExecStart=/usr/local/bin/kube-apiserver \\
          --advertise-address=${INTERNAL_IP} \\
          --allow-privileged=true \\
          --apiserver-count=2 \\
          --audit-log-maxage=30 \\
          --audit-log-maxbackup=3 \\
          --audit-log-maxsize=100 \\
          --audit-log-path=/var/log/audit.log \\
          --authorization-mode=Node,RBAC \\
          --bind-address=0.0.0.0 \\
          --client-ca-file=/var/lib/kubernetes/ca.crt \\
          --enable-admission-plugins=NodeRestriction,ServiceAccount \\
          --enable-swagger-ui=true \\
          --enable-bootstrap-token-auth=true \\
          --etcd-cafile=/var/lib/kubernetes/ca.crt \\
          --etcd-certfile=/var/lib/kubernetes/etcd-server.crt \\
          --etcd-keyfile=/var/lib/kubernetes/etcd-server.key \\
          --etcd-servers=https://192.168.42.1:2379,https://192.168.42.2:2379 \\
          --event-ttl=1h \\
          --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
          --kubelet-certificate-authority=/var/lib/kubernetes/ca.crt \\
          --kubelet-client-certificate=/var/lib/kubernetes/kube-apiserver.crt \\
          --kubelet-client-key=/var/lib/kubernetes/kube-apiserver.key \\
          --kubelet-https=true \\
          --runtime-config=api/all=true \\
          --service-account-key-file=/var/lib/kubernetes/service-account.crt \\
          --service-cluster-ip-range=10.96.0.0/24 \\
          --service-node-port-range=30000-32767 \\
          --tls-cert-file=/var/lib/kubernetes/kube-apiserver.crt \\
          --tls-private-key-file=/var/lib/kubernetes/kube-apiserver.key \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF
    -
      name: 'Copy kube-controller-manager kubeconfig to /var/lib/kubernetes'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        sudo cp kube-controller-manager.kubeconfig /var/lib/kubernetes/
    -
      name: 'Create kube-controller-manager service'
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
        [Unit]
        Description=Kubernetes Controller Manager
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        ExecStart=/usr/local/bin/kube-controller-manager \\
          --address=0.0.0.0 \\
          --cluster-cidr=192.168.42.0/24 \\
          --cluster-name=kubernetes \\
          --cluster-signing-cert-file=/var/lib/kubernetes/ca.crt \\
          --cluster-signing-key-file=/var/lib/kubernetes/ca.key \\
          --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
          --leader-elect=true \\
          --root-ca-file=/var/lib/kubernetes/ca.crt \\
          --service-account-private-key-file=/var/lib/kubernetes/service-account.key \\
          --service-cluster-ip-range=10.96.0.0/24 \\
          --use-service-account-credentials=true \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF
    -
      name: 'Copy kube-scheduler kubeconfig to /var/lib/kubernetes'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        sudo cp kube-scheduler.kubeconfig /var/lib/kubernetes/
    -
      name: 'Create kube-scheduler service'
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
        [Unit]
        Description=Kubernetes Scheduler
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        ExecStart=/usr/local/bin/kube-scheduler \\
          --kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\
          --address=127.0.0.1 \\
          --leader-elect=true \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF
    -
      name: 'Enable and start the Controller services'
      shell: |
        sudo systemctl daemon-reload
        sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
        sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
-
  name: 'Kubernetes Frontend Load Balancer'
  hosts: loadbalancer-1
  tasks: 
    -
      name: 'Install haproxy'
      shell: 'sudo apt update && sudo apt install -y haproxy'
    -
      name: 'Modify haproxy configuration file'
      shell: |
        cat <<EOF | sudo tee /etc/haproxy/haproxy.cfg 
        frontend kubernetes
            bind 192.168.42.21:6443
            option tcplog
            mode tcp
            default_backend kubernetes-master-nodes

        backend kubernetes-master-nodes
            mode tcp
            balance roundrobin
            option tcp-check
            server master-1 192.168.42.1:6443 check fall 3 rise 2
            server master-2 192.168.42.2:6443 check fall 3 rise 2
        EOF
        sudo service haproxy restart
-
  name: 'TLS Bootstrapping the Kubernetes Worker nodes'
  hosts: worker-*
  tasks: 
    -
      name: 'Install Kubernetes binaries for Worker nodes'
      shell: |
        sudo mkdir -p \
        /etc/cni/net.d \
        /opt/cni/bin \
        /var/lib/kubelet \
        /var/lib/kube-proxy \
        /var/lib/kubernetes \
        /var/run/kubernetes
        wget -q --show-progress --https-only --timestamping \
        https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubectl \
        https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy \
        https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubelet
        chmod +x kubectl kube-proxy kubelet
        sudo mv kubectl kube-proxy kubelet /usr/local/bin/
    -
      name: 'Copy CA certificate to /var/lib/kubernetes'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        sudo cp ca.crt /var/lib/kubernetes/
    -
      name: 'Create Bootstrap Token to be used by Worker nodes (kubelet) to invoke Certificate API'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        cat > bootstrap-token-07401b.yaml <<EOF
        apiVersion: v1
        kind: Secret
        metadata:
          # Name MUST be of form "bootstrap-token-<token id>"
          name: bootstrap-token-07401b
          namespace: kube-system

        # Type MUST be 'bootstrap.kubernetes.io/token'
        type: bootstrap.kubernetes.io/token
        stringData:
          # Human readable description. Optional.
          description: "The default bootstrap token generated by 'kubeadm init'."

          # Token ID and secret. Required.
          token-id: 07401b
          token-secret: f395accd246ae52d

          # Expiration. Optional.
          expiration: 2024-01-01T03:22:11Z

          # Allowed usages.
          usage-bootstrap-authentication: "true"
          usage-bootstrap-signing: "true"

          # Extra groups to authenticate the token as. Must start with "system:bootstrappers:"
          auth-extra-groups: system:bootstrappers:worker
        EOF
    -
      name: 'Apply Bootstrap Token'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        kubectl create -f bootstrap-token-07401b.yaml
      ignore_errors: true #If there is an error, it means the Bootstrap Token is already applied
      delegate_to: master-1
    -
      name: 'Authorize Worker nodes (kubelet) to create, approve and auto renew Certificates'
      shell: |
        kubectl create clusterrolebinding create-csrs-for-bootstrapping --clusterrole=system:node-bootstrapper --group=system:bootstrappers
        kubectl create clusterrolebinding auto-approve-csrs-for-group --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers
        kubectl create clusterrolebinding auto-approve-renewals-for-nodes --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes
      ignore_errors: true #If there is an error, it means Worker nodes can already create, approve and auto renew Certificates
      delegate_to: master-1
    -
      name: 'Configure kubelet to TLS Bootstrap'
      shell: |
        cat <<EOF | sudo tee /var/lib/kubelet/bootstrap-kubeconfig
        apiVersion: v1
        clusters:
        - cluster:
            certificate-authority: /var/lib/kubernetes/ca.crt
            server: https://192.168.42.21:6443
          name: bootstrap
        contexts:
        - context:
            cluster: bootstrap
            user: kubelet-bootstrap
          name: bootstrap
        current-context: bootstrap
        kind: Config
        preferences: {}
        users:
        - name: kubelet-bootstrap
          user:
            token: 07401b.f395accd246ae52d
        EOF
    -
      name: 'Create kubelet configuration file'
      shell: |
        cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
        kind: KubeletConfiguration
        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
          anonymous:
            enabled: false
          webhook:
            enabled: true
          x509:
            clientCAFile: "/var/lib/kubernetes/ca.crt"
        authorization:
          mode: Webhook
        cgroupDriver: systemd
        clusterDomain: "cluster.local"
        clusterDNS:
          - "10.96.0.10"
        resolvConf: "/run/systemd/resolve/resolv.conf"
        runtimeRequestTimeout: "15m"
        EOF
    -
      name: 'Create kubelet service'
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        After=docker.service
        Requires=docker.service

        [Service]
        ExecStart=/usr/local/bin/kubelet \\
          --bootstrap-kubeconfig="/var/lib/kubelet/bootstrap-kubeconfig" \\
          --config=/var/lib/kubelet/kubelet-config.yaml \\
          --image-pull-progress-deadline=2m \\
          --kubeconfig=/var/lib/kubelet/kubeconfig \\
          --cert-dir=/var/lib/kubelet/pki/ \\
          --rotate-certificates=true \\
          --rotate-server-certificates=true \\
          --network-plugin=cni \\
          --register-node=true \\
          --v=2
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF
    -
      name: 'Copy kube-proxy.kubeconfig to /var/lib/kube-proxy'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        sudo cp kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
    -
      name: 'Create kube-proxy configuration file'
      shell: |
        cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
        kind: KubeProxyConfiguration
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        clientConnection:
          kubeconfig: "/var/lib/kube-proxy/kubeconfig"
        mode: "iptables"
        clusterCIDR: "192.168.42.0/24"
        EOF
    -
      name: 'Create kube-proxy service'
      shell: |
        cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
        [Unit]
        Description=Kubernetes Kube Proxy
        Documentation=https://github.com/kubernetes/kubernetes

        [Service]
        ExecStart=/usr/local/bin/kube-proxy \\
          --config=/var/lib/kube-proxy/kube-proxy-config.yaml
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target
        EOF
    -
      name: 'Enable and start Worker services'
      shell: |
        sudo systemctl daemon-reload
        sudo systemctl enable kubelet kube-proxy
        sudo systemctl start kubelet kube-proxy
    -
      name: 'Approve Server CSR'
      shell: "kubectl certificate approve $(kubectl get csr|grep csr|awk -F ' ' '{print $1}')"
      delegate_to: master-1
-
  name: 'Configuring kubectl for Remote Access'
  hosts: master-*
  tasks:
    -
      name: 'Generate kubeconfig file for authenticating as the admin user'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        {
          LOADBALANCER_IP=192.168.42.21

          kubectl config set-cluster kubernetes-the-hard-way \
            --certificate-authority=ca.crt \
            --embed-certs=true \
            --server=https://${LOADBALANCER_IP}:6443

          kubectl config set-credentials admin \
            --client-certificate=admin.crt \
            --client-key=admin.key

          kubectl config set-context kubernetes-the-hard-way \
            --cluster=kubernetes-the-hard-way \
            --user=admin

          kubectl config use-context kubernetes-the-hard-way
        }
-
  name: 'Provisioning pod Network (CNI)'
  hosts: worker-*
  tasks:
    -
      name: 'Install CNI plugins required for Weave'
      shell: |
        wget https://github.com/containernetworking/plugins/releases/download/v0.7.5/cni-plugins-amd64-v0.7.5.tgz
        sudo tar -xzvf cni-plugins-amd64-v0.7.5.tgz --directory /opt/cni/bin/
    -
      name: 'Deploy Weave Network'
      shell: |
        kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
      ignore_errors: true #If there is an error, it means Weave Network is already deployed
      delegate_to: master-1
-
  name: 'RBAC for Kubelet Authorization'
  hosts: master-1
  tasks:
    -
      name: 'Create system:kube-apiserver-to-kubelet ClusterRole to access Kubelet API'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          annotations:
            rbac.authorization.kubernetes.io/autoupdate: "true"
          labels:
            kubernetes.io/bootstrapping: rbac-defaults
          name: system:kube-apiserver-to-kubelet
        rules:
          - apiGroups:
              - ""
            resources:
              - nodes/proxy
              - nodes/stats
              - nodes/log
              - nodes/spec
              - nodes/metrics
            verbs:
              - "*"
        EOF
    -
      name: 'Bind the system:kube-apiserver-to-kubelet ClusterRole to the system:kube-apiserver user'
      shell: |
        cd /vagrant/.vagrant/shared-folder
        cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: system:kube-apiserver
          namespace: ""
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:kube-apiserver-to-kubelet
        subjects:
          - apiGroup: rbac.authorization.k8s.io
            kind: User
            name: kube-apiserver
        EOF
-
  name: 'Deploying DNS Cluster Add-on'
  hosts: master-1
  tasks:
    -
      name: 'Deploy coredns Add-on'
      shell: |
        kubectl apply -f https://raw.githubusercontent.com/mmumshad/kubernetes-the-hard-way/master/deployments/coredns.yaml
